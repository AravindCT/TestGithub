# Load necessary libraries
!pip install transformers datasets
!pip install gradio
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW
from torch.nn import CrossEntropyLoss
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
import torch
import gradio as gr

# Load the dataset without streaming
dataset = load_dataset('mshojaei77/merged_mental_health_dataset')

# Split the dataset into training and validation sets
dataset_splits = dataset['train'].train_test_split(test_size=0.1)
dataset = DatasetDict({
    'train': dataset_splits['train'],
    'validation': dataset_splits['test']
})

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')
tokenizer.pad_token = tokenizer.eos_token  # Set padding token

# Define the tokenization function
def tokenize_function(batch):
    contexts = []
    for context in batch['Context']:
        if isinstance(context, list):
            ctx = " ".join([str(x) for x in context if x is not None])
        else:
            ctx = str(context)
        contexts.append(ctx)
    responses = []
    for response in batch['Response']:
        if isinstance(response, list):
            res = " ".join([str(x) for x in response if x is not None])
        else:
            res = str(response)
        responses.append(res)
    texts = [ctx + tokenizer.eos_token + res for ctx, res in zip(contexts, responses)]
    tokenized = tokenizer(texts, truncation=True, padding='max_length', max_length=512)
    return tokenized

# Map the tokenization function to the dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True, batch_size=8)

# Create DataLoaders
train_dataloader = DataLoader(tokenized_datasets['train'], batch_size=8, shuffle=True)
val_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=8)

# Initialize model, optimizer, and loss function
model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')
optimizer = AdamW(model.parameters(), lr=5e-5)
loss_fn = CrossEntropyLoss()

# Training loop with gradient accumulation and mixed-precision training
accumulation_steps = 4
num_epochs = 3
scaler = GradScaler()

try:
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        for i, batch in enumerate(train_dataloader):
            optimizer.zero_grad()
            with autocast():
                outputs = model(**batch)
                logits = outputs.logits
                labels = batch['input_ids']
                loss = loss_fn(logits.view(-1, model.config.vocab_size), labels.view(-1))
            scaler.scale(loss).backward()
            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()
            if i % 100 == 0:
                print(f"Epoch {epoch}, Step {i}, Loss: {loss.item()}")
        model.eval()
        with torch.no_grad():
            for batch in val_dataloader:
                with autocast():
                    outputs = model(**batch)
                    logits = outputs.logits
                    labels = batch['input_ids']
                    loss = loss_fn(logits.view(-1, model.config.vocab_size), labels.view(-1))
except Exception as e:
    print(f"An error occurred: {e}")

# Chatbot function with conversation history management
conversation_history = []

def chatbot(input_text):
    conversation_history.append(input_text)
    if len(conversation_history) > 10:
        conversation_history.pop(0)
    inputs = tokenizer.encode(' '.join(conversation_history) + tokenizer.eos_token, return_tensors='pt')
    response = model.generate(inputs, max_length=512, do_sample=True)
    response_text = tokenizer.decode(response[0], skip_special_tokens=True)
    conversation_history.append(response_text)
    response_text += "\n\n*This is a chatbot and not a substitute for professional advice.*"
    return response_text

# Deploy the chatbot using Gradio
demo = gr.Interface(fn=chatbot, inputs="text", outputs="text")
demo.launch()

# Check library versions
print(f"PyTorch version: {torch.__version__}")
print(f"Transformers version: {transformers.__version__}")
print(f"Datasets version: {datasets.__version__}")
